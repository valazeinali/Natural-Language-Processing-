{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import theano\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us load our data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = full_df[[\"comment\"]]\n",
    "df[\"comment\"] = df[\"comment\"].astype(str) #declaring the contents in the col --\n",
    "#--> \"comment\" as a string variable.\n",
    "\n",
    "full_df.head() # lets see what our rows and cols look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order for us to model language and predict sarcasm, we must first wrangle and clean our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will first start with lower casing our \"text\". This will treat the string 'text', 'Text', 'TEXT' homogeneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>nc and nh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>you do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>they were underdogs earlier today, but since g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>this meme isn't funny none of the \"new york ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>i could use one of those tools.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                                         NC and NH.   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today, but since G...   \n",
       "3  This meme isn't funny none of the \"new york ni...   \n",
       "4                    I could use one of those tools.   \n",
       "\n",
       "                                          text_lower  \n",
       "0                                         nc and nh.  \n",
       "1  you do know west teams play against west teams...  \n",
       "2  they were underdogs earlier today, but since g...  \n",
       "3  this meme isn't funny none of the \"new york ni...  \n",
       "4                    i could use one of those tools.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_lower\"] = df[\"comment\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will remove the following symbols \n",
    " #### --> string.punctuation includes #$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the new column created in last cell\n",
    "df.drop([\"text_lower\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_wo_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>NC and NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They were underdogs earlier today but since Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>This meme isnt funny none of the new york nigg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>I could use one of those tools</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                                         NC and NH.   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today, but since G...   \n",
       "3  This meme isn't funny none of the \"new york ni...   \n",
       "4                    I could use one of those tools.   \n",
       "\n",
       "                                       text_wo_punct  \n",
       "0                                          NC and NH  \n",
       "1  You do know west teams play against west teams...  \n",
       "2  They were underdogs earlier today but since Gr...  \n",
       "3  This meme isnt funny none of the new york nigg...  \n",
       "4                     I could use one of those tools  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(comment):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return comment.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"text_wo_punct\"] = df[\"comment\"].apply(lambda comment: remove_punctuation(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will remove stop words like \"the\", \"a\", \"I\" to help with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valazeinali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>NC and NH</td>\n",
       "      <td>NC NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You know west teams play west teams east teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They were underdogs earlier today but since Gr...</td>\n",
       "      <td>They underdogs earlier today since Gronks anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>This meme isnt funny none of the new york nigg...</td>\n",
       "      <td>This meme isnt funny none new york nigga ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>I could use one of those tools</td>\n",
       "      <td>I could use one tools</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                                         NC and NH.   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today, but since G...   \n",
       "3  This meme isn't funny none of the \"new york ni...   \n",
       "4                    I could use one of those tools.   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0                                          NC and NH   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today but since Gr...   \n",
       "3  This meme isnt funny none of the new york nigg...   \n",
       "4                     I could use one of those tools   \n",
       "\n",
       "                                        text_wo_stop  \n",
       "0                                              NC NH  \n",
       "1  You know west teams play west teams east teams...  \n",
       "2  They underdogs earlier today since Gronks anno...  \n",
       "3      This meme isnt funny none new york nigga ones  \n",
       "4                              I could use one tools  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(comment):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(comment).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda comment: remove_stopwords(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see what some frequent words are and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 169221),\n",
       " ('like', 53550),\n",
       " ('dont', 36745),\n",
       " ('people', 34167),\n",
       " ('would', 34048),\n",
       " ('Yeah', 33124),\n",
       " ('get', 32644),\n",
       " ('Im', 30655),\n",
       " ('one', 29323),\n",
       " ('But', 27309)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for comment in df[\"text_wo_stop\"].values:\n",
    "    for word in comment.split(): # going through every word in every tweet\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)\n",
    "        \n",
    "        #### Changes made by *VALA ####\n",
    "        ## Lets make Histogram of most frequent words\n",
    "    #y_pos = word\n",
    "    #freq = cnt[word] #get the freq of the words\n",
    "        \n",
    "    #plt.bar(y_pos,freq, align = 'center', alpha = .5)\n",
    "    #plt.xticks(y_pos, word)\n",
    "    #plt.ylabel('Frequent Words')\n",
    "    #plt.title('Most frequent words in our Twitter data set')\n",
    "        \n",
    "    #plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_wo_punct</th>\n",
       "      <th>text_wo_stop</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>NC and NH</td>\n",
       "      <td>NC NH</td>\n",
       "      <td>NC NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You know west teams play west teams east teams...</td>\n",
       "      <td>You know west teams play west teams east teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They were underdogs earlier today but since Gr...</td>\n",
       "      <td>They underdogs earlier today since Gronks anno...</td>\n",
       "      <td>They underdogs earlier today since Gronks anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>This meme isnt funny none of the new york nigg...</td>\n",
       "      <td>This meme isnt funny none new york nigga ones</td>\n",
       "      <td>This meme isnt funny none new york nigga ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>I could use one of those tools</td>\n",
       "      <td>I could use one tools</td>\n",
       "      <td>could use tools</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                                         NC and NH.   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today, but since G...   \n",
       "3  This meme isn't funny none of the \"new york ni...   \n",
       "4                    I could use one of those tools.   \n",
       "\n",
       "                                       text_wo_punct  \\\n",
       "0                                          NC and NH   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today but since Gr...   \n",
       "3  This meme isnt funny none of the new york nigg...   \n",
       "4                     I could use one of those tools   \n",
       "\n",
       "                                        text_wo_stop  \\\n",
       "0                                              NC NH   \n",
       "1  You know west teams play west teams east teams...   \n",
       "2  They underdogs earlier today since Gronks anno...   \n",
       "3      This meme isnt funny none new york nigga ones   \n",
       "4                              I could use one tools   \n",
       "\n",
       "                                    text_wo_stopfreq  \n",
       "0                                              NC NH  \n",
       "1  You know west teams play west teams east teams...  \n",
       "2  They underdogs earlier today since Gronks anno...  \n",
       "3      This meme isnt funny none new york nigga ones  \n",
       "4                                    could use tools  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(comment):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(comment).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda comment: remove_freqwords(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets remove some rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>text_wo_stopfreq</th>\n",
       "      <th>text_wo_stopfreqrare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>NC NH</td>\n",
       "      <td>NC NH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>You know west teams play west teams east teams...</td>\n",
       "      <td>You know west teams play west teams east teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>They underdogs earlier today since Gronks anno...</td>\n",
       "      <td>They underdogs earlier today since Gronks anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>This meme isnt funny none new york nigga ones</td>\n",
       "      <td>This meme isnt funny none new york nigga ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>could use tools</td>\n",
       "      <td>could use tools</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                                         NC and NH.   \n",
       "1  You do know west teams play against west teams...   \n",
       "2  They were underdogs earlier today, but since G...   \n",
       "3  This meme isn't funny none of the \"new york ni...   \n",
       "4                    I could use one of those tools.   \n",
       "\n",
       "                                    text_wo_stopfreq  \\\n",
       "0                                              NC NH   \n",
       "1  You know west teams play west teams east teams...   \n",
       "2  They underdogs earlier today since Gronks anno...   \n",
       "3      This meme isnt funny none new york nigga ones   \n",
       "4                                    could use tools   \n",
       "\n",
       "                                text_wo_stopfreqrare  \n",
       "0                                              NC NH  \n",
       "1  You know west teams play west teams east teams...  \n",
       "2  They underdogs earlier today since Gronks anno...  \n",
       "3      This meme isnt funny none new york nigga ones  \n",
       "4                                    could use tools  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the two columns which are no more needed \n",
    "df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(comment):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(comment).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda comment: remove_rarewords(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets stem our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Drop the two columns \n",
    "#df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(comment):\n",
    "    return \" \".join([stemmer.stem(word) for word in comment.split()])\n",
    "\n",
    "df[\"text_stemmed\"] = df[\"comment\"].apply(lambda comment: stem_words(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that words like private and propose have their e at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n",
    "\n",
    "## Also this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages ## we can use snowballstemmer \n",
    "#for all these languages, WOAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use lemmatization to make sure the root word in preserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"comment\"].apply(lambda comment: lemmatize_words(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the trailing e in the propose and private is retained when we use lemmatization unlike stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(comment):\n",
    "    pos_tagged_text = nltk.pos_tag(comment.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"comment\"].apply(lambda comment: lemmatize_words(comment))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert emoticons to english \n",
    "### Ex: :-) to Happy_face_smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons(comment):\n",
    "    for emot in EMOTICONS:\n",
    "        comment = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert emojis to words\n",
    "### Ex:  🔥to fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(comment):\n",
    "    for emot in UNICODE_EMO:\n",
    "        comment = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us remove URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(comment):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(comment):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets do some sarcasm detection\n",
    "\n",
    "## First we will apply all the data cleaning to get a data frame that's reeady for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each dataframe update takes the previous data frame and updates it to \n",
    "#the current function action.\n",
    "\n",
    "#lhs = new dataframe , rhs= old dataframe\n",
    "\n",
    "#below the lhs is the text but all lowercase, the rhs is the \n",
    "#original tweets in the same format\n",
    "full_df[\"text_lower\"] = full_df[\"comment\"].str.lower()\n",
    "\n",
    "#remove punctuation \n",
    "full_df[\"text_wo_punct\"] = full_df[\"text_lower\"].apply(lambda comment: remove_punctuation(comment))\n",
    "\n",
    "#remove rare words\n",
    "full_df[\"text_wo_stopfreqrare\"] = full_df[\"text_wo_punct\"].apply(lambda comment: remove_rarewords(comment))\n",
    "\n",
    "#remove frequent words\n",
    "full_df[\"text_wo_stopfreq\"] = full_df[\"text_wo_stopfreqrare\"].apply(lambda comment: remove_freqwords(comment))\n",
    "\n",
    "#lemmatize\n",
    "full_df[\"text_lemmatized2\"] = full_df[\"text_wo_stopfreq\"].apply(lambda comment: lemmatize_words(comment))\n",
    "\n",
    "## below we will convert emoticons like :-) to Happy_face_smiley and etc\n",
    "#df[\"text_emotconsless\"] = df[\"text_lemmatized2\"].apply(lambda text: convert_emoticons(text))\n",
    "\n",
    "## convert emojis to english text\n",
    "#df[\"text_emojiless\"] = df[\"text_emotconsless\"].apply(lambda text: convert_emojis(text))\n",
    "\n",
    "#remove URls\n",
    "full_df[\"URL_gone\"] = full_df[\"text_lemmatized2\"].apply(lambda comment: remove_urls(comment))\n",
    "                                           \n",
    "#remove HTML tags\n",
    "full_df[\"final_df\"] = full_df[\"URL_gone\"].apply(lambda comment: remove_urls(comment))\n",
    "\n",
    "\n",
    "full_df.shape\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for non data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(subset=['final_df'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
